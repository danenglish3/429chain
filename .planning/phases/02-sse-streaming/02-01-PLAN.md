---
phase: 02-sse-streaming
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/shared/types.ts
  - src/providers/types.ts
  - src/providers/base-adapter.ts
  - src/streaming/sse-parser.ts
autonomous: true

must_haves:
  truths:
    - "BaseAdapter can open a streaming fetch to any provider and return the raw Response with a ReadableStream body"
    - "SSE chunks from upstream providers are correctly parsed into individual data payloads"
    - "The streaming request body sets stream: true and forwards AbortSignal to fetch"
  artifacts:
    - path: "src/shared/types.ts"
      provides: "ChatCompletionChunk type for streaming response chunks"
      contains: "ChatCompletionChunk"
    - path: "src/providers/types.ts"
      provides: "chatCompletionStream method on ProviderAdapter interface"
      contains: "chatCompletionStream"
    - path: "src/providers/base-adapter.ts"
      provides: "Concrete chatCompletionStream implementation with streaming fetch"
      contains: "chatCompletionStream"
    - path: "src/streaming/sse-parser.ts"
      provides: "SSE chunk parser that handles OpenAI data: format and [DONE] marker"
      exports: ["parseSSEChunk"]
  key_links:
    - from: "src/providers/base-adapter.ts"
      to: "fetch()"
      via: "chatCompletionStream passes signal to fetch and sets stream:true in body"
      pattern: "stream.*true"
    - from: "src/providers/base-adapter.ts"
      to: "src/providers/types.ts"
      via: "implements ProviderAdapter.chatCompletionStream"
      pattern: "chatCompletionStream"
---

<objective>
Add streaming infrastructure to the provider adapter layer: a `chatCompletionStream()` method on BaseAdapter that returns a raw Response with ReadableStream body, OpenAI streaming chunk types, and an SSE parser utility.

Purpose: This is the foundation for SSE streaming. The route handler (Plan 02) needs adapters that can open streaming connections and a parser that can extract individual SSE events from the byte stream. Without these, no streaming can happen.

Output: Modified BaseAdapter with chatCompletionStream(), updated ProviderAdapter interface, ChatCompletionChunk type, and parseSSEChunk utility.
</objective>

<execution_context>
@C:\Users\danen\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\danen\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-sse-streaming/02-RESEARCH.md

@src/providers/base-adapter.ts
@src/providers/types.ts
@src/shared/types.ts
@src/shared/errors.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add streaming types and ProviderAdapter interface update</name>
  <files>src/shared/types.ts, src/providers/types.ts</files>
  <action>
1. In `src/shared/types.ts`, add the OpenAI streaming chunk types AFTER the existing `ChatCompletionResponse` interface:

```typescript
/** A single delta in a streaming chunk choice. */
export interface ChatCompletionDelta {
  role?: 'assistant';
  content?: string | null;
  tool_calls?: ToolCall[];
}

/** A single choice in a streaming chunk. */
export interface ChatCompletionChunkChoice {
  index: number;
  delta: ChatCompletionDelta;
  finish_reason: 'stop' | 'length' | 'tool_calls' | 'content_filter' | null;
}

/** OpenAI-compatible streaming chunk (one SSE event payload). */
export interface ChatCompletionChunk {
  id: string;
  object: 'chat.completion.chunk';
  created: number;
  model: string;
  choices: ChatCompletionChunkChoice[];
  usage?: Usage | null;
  system_fingerprint?: string;
}
```

2. In `src/providers/types.ts`, add the `chatCompletionStream` method to the `ProviderAdapter` interface, right after the existing `chatCompletion` method:

```typescript
/**
 * Send a streaming chat completion request.
 * Returns the raw fetch Response so the caller can read the ReadableStream body.
 * @param model - The model ID to request.
 * @param body - The chat completion request body (model field is overridden).
 * @param signal - Optional AbortSignal for request cancellation (critical for cleanup).
 * @returns Raw fetch Response with body as ReadableStream of SSE chunks.
 * @throws ProviderRateLimitError on 429 responses (before stream starts).
 * @throws ProviderError on other non-OK responses.
 */
chatCompletionStream(
  model: string,
  body: ChatCompletionRequest,
  signal?: AbortSignal,
): Promise<Response>;
```

Import note: `Response` is a global type in Node 18+, no import needed.
  </action>
  <verify>Run `npx tsc --noEmit` -- should compile with no errors (BaseAdapter will error until Task 2, that's expected if running verify after both tasks).</verify>
  <done>ChatCompletionChunk and related types exist in shared/types.ts. ProviderAdapter interface includes chatCompletionStream method signature.</done>
</task>

<task type="auto">
  <name>Task 2: Implement chatCompletionStream on BaseAdapter</name>
  <files>src/providers/base-adapter.ts</files>
  <action>
Add a `chatCompletionStream` method to `BaseAdapter`, placed after the existing `chatCompletion` method. This method:

1. Builds the URL the same way as `chatCompletion` (`${this.baseUrl}/chat/completions`)
2. Calls `this.prepareRequestBody(model, body)` but then OVERRIDES `stream` to `true` (the existing prepareRequestBody sets `stream: false` -- we must override after calling it)
3. Builds headers identically to `chatCompletion` (Content-Type, Authorization, getExtraHeaders)
4. Calls `fetch()` with method POST, headers, JSON body, and forwards the `signal` parameter
5. On 429: reads response text and throws `ProviderRateLimitError` (same as chatCompletion)
6. On non-OK: reads response text and throws `ProviderError` (same as chatCompletion)
7. On success: returns the raw `Response` object directly -- the caller reads `.body` as a ReadableStream

Key differences from chatCompletion:
- Sets `stream: true` in the request body (not false)
- Does NOT read the response body (returns raw Response so the ReadableStream is unconsumed)
- Does NOT measure latency (latency is measured by the streaming route handler across the full stream)
- Logs at debug level: "Starting streaming request" on start, no success log (route handles that)

Implementation:

```typescript
async chatCompletionStream(
  model: string,
  body: ChatCompletionRequest,
  signal?: AbortSignal,
): Promise<Response> {
  const url = `${this.baseUrl}/chat/completions`;
  const requestBody = this.prepareRequestBody(model, body);
  requestBody.stream = true; // Override: force streaming

  const headers: Record<string, string> = {
    'Content-Type': 'application/json',
    'Authorization': `Bearer ${this.apiKey}`,
    ...this.getExtraHeaders(),
  };

  logger.debug({ provider: this.id, model, url }, 'Starting streaming request');

  const response = await fetch(url, {
    method: 'POST',
    headers,
    body: JSON.stringify(requestBody),
    signal,
  });

  if (response.status === 429) {
    const responseBody = await response.text();
    logger.warn(
      { provider: this.id, model },
      'Provider returned 429 rate limit (streaming)',
    );
    throw new ProviderRateLimitError(this.id, model, response.headers, responseBody);
  }

  if (!response.ok) {
    const errorText = await response.text();
    logger.error(
      { provider: this.id, model, status: response.status },
      'Streaming request failed',
    );
    throw new ProviderError(this.id, model, response.status, errorText);
  }

  return response;
}
```

Note: `prepareRequestBody` returns `Record<string, unknown>` so setting `.stream = true` on the result is valid. The original body's `stream` field is already removed by `prepareRequestBody` (it spreads `rest` which excludes nothing beyond model -- actually it spreads ALL fields including stream). Looking at the code: `const { model: _originalModel, ...rest } = body;` then `return { ...rest, model, stream: false }`. So the original `stream` in `rest` is overridden by `stream: false`. Our override `requestBody.stream = true` then overrides that. This works correctly.
  </action>
  <verify>Run `npx tsc --noEmit` -- should compile cleanly. The method signature matches the interface added in Task 1.</verify>
  <done>BaseAdapter.chatCompletionStream() exists, compiles, sends stream:true in body, forwards AbortSignal to fetch, throws on 429/error, returns raw Response on success.</done>
</task>

<task type="auto">
  <name>Task 3: Create SSE chunk parser utility</name>
  <files>src/streaming/sse-parser.ts</files>
  <action>
Create a new file `src/streaming/sse-parser.ts` that exports an SSE parser for the OpenAI streaming format.

The OpenAI SSE format is:
- Each event: `data: {json_or_done}\n\n`
- Stream terminator: `data: [DONE]\n\n`
- Events are separated by double newlines
- A single TCP read may contain multiple events, or a partial event

The parser must handle:
1. **Multiple events in one chunk** (split on `\n\n`)
2. **Partial events across chunks** (buffer incomplete data until next chunk)
3. **The [DONE] marker** (signal stream end)
4. **Empty lines and comment lines** (lines starting with `:` are SSE comments/keepalives, skip them)

Implementation:

```typescript
/**
 * SSE stream parser for OpenAI-compatible streaming responses.
 * Handles buffering of partial chunks across TCP reads.
 */

/** Result of parsing an SSE chunk. */
export interface SSEParseResult {
  /** Complete SSE data payloads (JSON strings, NOT including [DONE]). */
  events: string[];
  /** True if [DONE] marker was encountered. */
  done: boolean;
}

/**
 * Create a stateful SSE parser that handles partial chunks.
 * Call parse() for each chunk received from the ReadableStream.
 * The parser buffers incomplete events across calls.
 *
 * @returns Object with parse() method.
 */
export function createSSEParser(): { parse(chunk: string): SSEParseResult } {
  let buffer = '';

  return {
    parse(chunk: string): SSEParseResult {
      buffer += chunk;
      const events: string[] = [];
      let done = false;

      // Split on double newline (SSE event boundary)
      const parts = buffer.split('\n\n');

      // Last part may be incomplete -- keep it in buffer
      buffer = parts.pop() ?? '';

      for (const part of parts) {
        const trimmed = part.trim();
        if (!trimmed) continue; // Empty segment
        if (trimmed.startsWith(':')) continue; // SSE comment / keepalive

        // Extract data from "data: ..." lines
        // An SSE event can have multiple lines; we care about data: lines
        const lines = trimmed.split('\n');
        for (const line of lines) {
          if (line.startsWith('data: ')) {
            const data = line.slice(6); // Remove "data: " prefix
            if (data === '[DONE]') {
              done = true;
            } else {
              events.push(data);
            }
          }
          // Ignore event:, id:, retry: fields (not used by OpenAI format)
        }
      }

      return { events, done };
    },
  };
}
```

Create the `src/streaming/` directory if it does not exist. This is a new module for all streaming-related utilities.

Also create `src/streaming/index.ts` as a barrel export:

```typescript
export { createSSEParser, type SSEParseResult } from './sse-parser.js';
```
  </action>
  <verify>Run `npx tsc --noEmit` -- should compile cleanly. Verify the file exists with: `ls src/streaming/sse-parser.ts`</verify>
  <done>SSE parser utility exists at src/streaming/sse-parser.ts. It handles multiple events per chunk, partial buffering, [DONE] detection, and SSE comments. Barrel export exists at src/streaming/index.ts.</done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` compiles with zero errors
2. `ChatCompletionChunk` type exists in src/shared/types.ts
3. `ProviderAdapter` interface includes `chatCompletionStream` method
4. `BaseAdapter.chatCompletionStream()` sets `stream: true`, forwards signal, handles 429/errors, returns raw Response
5. `createSSEParser()` is exported from src/streaming/sse-parser.ts and src/streaming/index.ts
</verification>

<success_criteria>
- TypeScript compiles with no errors
- chatCompletionStream exists on BaseAdapter and ProviderAdapter interface
- SSE parser handles: multi-event chunks, partial buffering, [DONE] marker, SSE comments
- No new dependencies added (zero npm installs)
- All existing code continues to work (prepareRequestBody stream:false unchanged for non-streaming)
</success_criteria>

<output>
After completion, create `.planning/phases/02-sse-streaming/02-01-SUMMARY.md`
</output>
