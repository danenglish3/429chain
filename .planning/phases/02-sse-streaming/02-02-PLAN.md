---
phase: 02-sse-streaming
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - src/chain/router.ts
  - src/api/routes/chat.ts
autonomous: true

must_haves:
  truths:
    - "A stream:true request returns text/event-stream with real-time SSE chunks and no buffering"
    - "Exhausted providers are skipped before the stream opens -- the proxy returns 503 JSON if all exhausted, not an empty stream"
    - "A 429 during pre-stream waterfall causes fallthrough to the next provider, transparent to the client"
    - "Client disconnect aborts the upstream fetch and releases the connection"
    - "Mid-stream errors produce an error SSE event and close gracefully without crashing the server"
    - "Non-streaming requests continue to work exactly as before"
  artifacts:
    - path: "src/chain/router.ts"
      provides: "executeStreamChain function for pre-stream waterfall validation"
      exports: ["executeStreamChain"]
      contains: "executeStreamChain"
    - path: "src/api/routes/chat.ts"
      provides: "Streaming branch in POST /chat/completions using streamSSE"
      contains: "streamSSE"
  key_links:
    - from: "src/api/routes/chat.ts"
      to: "src/chain/router.ts"
      via: "calls executeStreamChain for pre-stream provider selection"
      pattern: "executeStreamChain"
    - from: "src/api/routes/chat.ts"
      to: "hono/streaming"
      via: "imports and calls streamSSE() for SSE response"
      pattern: "streamSSE"
    - from: "src/api/routes/chat.ts"
      to: "src/providers/base-adapter.ts"
      via: "calls adapter.chatCompletionStream() inside streamSSE callback"
      pattern: "chatCompletionStream"
    - from: "src/api/routes/chat.ts"
      to: "src/streaming/sse-parser.ts"
      via: "uses createSSEParser to parse upstream SSE chunks"
      pattern: "createSSEParser"
    - from: "src/api/routes/chat.ts"
      to: "AbortController"
      via: "stream.onAbort() triggers abortController.abort() to cancel upstream fetch"
      pattern: "onAbort.*abort"
---

<objective>
Wire streaming end-to-end: add a pre-stream waterfall function to the chain router, then implement the streaming branch in the chat route handler using Hono's `streamSSE()` with AbortController cleanup.

Purpose: This completes the SSE streaming feature. After this plan, `stream: true` requests flow through the proxy with real-time token delivery, pre-stream provider validation, client disconnect cleanup, and graceful error handling. This satisfies all three Phase 2 success criteria.

Output: Modified chain router with executeStreamChain(), modified chat route with streaming branch.
</objective>

<execution_context>
@C:\Users\danen\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\danen\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-sse-streaming/02-RESEARCH.md
@.planning/phases/02-sse-streaming/02-01-SUMMARY.md

@src/chain/router.ts
@src/chain/types.ts
@src/api/routes/chat.ts
@src/providers/base-adapter.ts
@src/providers/types.ts
@src/shared/types.ts
@src/shared/errors.ts
@src/streaming/sse-parser.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add pre-stream waterfall function to chain router</name>
  <files>src/chain/router.ts, src/chain/types.ts</files>
  <action>
Add a new exported function `executeStreamChain` to `src/chain/router.ts`. This function performs the **pre-stream waterfall**: it iterates chain entries, skips exhausted ones, and for the first available provider, attempts to open a streaming connection. If that provider returns 429, it marks it exhausted and tries the next. If all fail, it throws AllProvidersExhaustedError.

The key difference from `executeChain`:
- Returns a `StreamChainResult` (raw Response + metadata) instead of `ChainResult` (parsed body)
- Calls `adapter.chatCompletionStream()` instead of `adapter.chatCompletion()`
- On 429: marks exhausted and waterfalls (same as executeChain)
- On other ProviderError: waterfalls (same as executeChain)
- On success: returns immediately with the raw Response (stream not yet consumed)

1. First, add the `StreamChainResult` type to `src/chain/types.ts`:

```typescript
/** Result of a successful streaming chain execution (pre-stream phase). */
export interface StreamChainResult {
  /** Raw fetch Response with unconsumed ReadableStream body. */
  response: Response;
  /** The provider that opened the stream. */
  providerId: string;
  /** The model being streamed. */
  model: string;
  /** Record of all attempts made before finding a working provider. */
  attempts: AttemptRecord[];
}
```

2. Then add `executeStreamChain` to `src/chain/router.ts`:

```typescript
import type { Chain, ChainResult, StreamChainResult } from './types.js';

/**
 * Execute a chain for streaming: iterate entries in order, skip exhausted,
 * waterfall on failure, return the first successful raw streaming Response.
 *
 * This performs PRE-STREAM validation: the provider connection is opened
 * and validated (non-429, non-error) BEFORE returning to the caller.
 * The caller then pipes the ReadableStream body to the client.
 */
export async function executeStreamChain(
  chain: Chain,
  request: ChatCompletionRequest,
  tracker: RateLimitTracker,
  registry: ProviderRegistry,
  signal?: AbortSignal,
): Promise<StreamChainResult> {
  const attempts: AttemptRecord[] = [];

  for (const entry of chain.entries) {
    if (tracker.isExhausted(entry.providerId, entry.model)) {
      logger.info(
        { provider: entry.providerId, model: entry.model, chain: chain.name },
        `Skipping ${entry.providerId}/${entry.model} (on cooldown) [stream]`,
      );
      attempts.push({
        provider: entry.providerId,
        model: entry.model,
        error: 'on_cooldown',
        skipped: true,
      });
      continue;
    }

    const adapter = registry.get(entry.providerId);

    try {
      const response = await adapter.chatCompletionStream(entry.model, request, signal);

      logger.info(
        {
          provider: entry.providerId,
          model: entry.model,
          chain: chain.name,
          attemptsCount: attempts.length + 1,
        },
        `Stream opened from ${entry.providerId}/${entry.model} (${attempts.length + 1} attempt(s))`,
      );

      return {
        response,
        providerId: entry.providerId,
        model: entry.model,
        attempts,
      };
    } catch (error: unknown) {
      if (error instanceof ProviderRateLimitError) {
        const retryAfterHeader = error.headers.get('retry-after');
        let retryAfterMs: number | undefined;
        if (retryAfterHeader) {
          const seconds = parseInt(retryAfterHeader, 10);
          if (!isNaN(seconds)) {
            retryAfterMs = seconds * 1000;
          }
        }

        tracker.markExhausted(
          entry.providerId,
          entry.model,
          retryAfterMs,
          '429 rate limited (streaming)',
        );

        logger.info(
          { provider: entry.providerId, model: entry.model, chain: chain.name, retryAfterMs },
          `Provider ${entry.providerId}/${entry.model} returned 429 [stream], waterfalling`,
        );

        attempts.push({
          provider: entry.providerId,
          model: entry.model,
          error: '429_rate_limited',
          retryAfter: retryAfterMs,
        });
        continue;
      }

      if (error instanceof ProviderError) {
        logger.info(
          { provider: entry.providerId, model: entry.model, chain: chain.name, statusCode: error.statusCode },
          `Provider ${entry.providerId}/${entry.model} returned ${error.statusCode} [stream], waterfalling`,
        );
        attempts.push({
          provider: entry.providerId,
          model: entry.model,
          error: `${error.statusCode}: ${error.message}`,
        });
        continue;
      }

      const errorMessage = error instanceof Error ? error.message : String(error);

      // If this is an AbortError, the client disconnected -- don't waterfall, just throw
      if (error instanceof Error && error.name === 'AbortError') {
        throw error;
      }

      logger.info(
        { provider: entry.providerId, model: entry.model, chain: chain.name, error: errorMessage },
        `Provider ${entry.providerId}/${entry.model} failed [stream]: ${errorMessage}, waterfalling`,
      );
      attempts.push({
        provider: entry.providerId,
        model: entry.model,
        error: errorMessage,
      });
      continue;
    }
  }

  logger.warn(
    { chain: chain.name, totalAttempts: attempts.length, attempts },
    `All providers exhausted in chain "${chain.name}" [stream]`,
  );

  throw new AllProvidersExhaustedError(chain.name, attempts);
}
```

Make sure to add the `ChatCompletionRequest` import to router.ts if not already present (it should be imported from `../shared/types.js`).

Note the `signal` parameter: it is forwarded to `adapter.chatCompletionStream()`. If the client disconnects during the pre-stream waterfall phase, the AbortError propagates up and the route handler catches it cleanly.
  </action>
  <verify>Run `npx tsc --noEmit` -- should compile cleanly with new types and function.</verify>
  <done>executeStreamChain exists in src/chain/router.ts. StreamChainResult type exists in src/chain/types.ts. Pre-stream waterfall skips exhausted providers, handles 429 with fallthrough, propagates AbortError, throws AllProvidersExhaustedError when all fail.</done>
</task>

<task type="auto">
  <name>Task 2: Wire streaming branch in chat route with SSE, abort cleanup, and error handling</name>
  <files>src/api/routes/chat.ts</files>
  <action>
Modify `src/api/routes/chat.ts` to handle `stream: true` requests using Hono's `streamSSE()` helper, with AbortController wiring and client disconnect cleanup.

Changes to make:

1. Add imports at the top:
```typescript
import { streamSSE } from 'hono/streaming';
import { executeStreamChain } from '../../chain/router.js';
import { createSSEParser } from '../../streaming/sse-parser.js';
import { AllProvidersExhaustedError } from '../../shared/errors.js';
```

2. Replace the current streaming warning + fallback with a real streaming branch. The route handler should:
   - Parse the request body
   - Resolve the chain (same as current)
   - If `body.stream` is truthy, enter the streaming branch
   - If not, continue with existing non-streaming logic (keep it unchanged)

3. The streaming branch implementation:

```typescript
if (body.stream) {
  // Strip model field (chain entry determines actual model)
  const { model: _model, ...streamBody } = body;

  // Create AbortController for upstream cleanup
  const abortController = new AbortController();

  // Pre-stream waterfall: find available provider and open stream
  // This happens BEFORE streamSSE() -- if all exhausted, return JSON error
  let streamResult;
  try {
    streamResult = await executeStreamChain(
      chain,
      streamBody as ChatCompletionRequest,
      tracker,
      registry,
      abortController.signal,
    );
  } catch (error) {
    if (error instanceof AllProvidersExhaustedError) {
      return c.json(error.toOpenAIError(), 503);
    }
    throw error;
  }

  // Set informational headers
  c.header('X-429chain-Provider', `${streamResult.providerId}/${streamResult.model}`);
  c.header('X-429chain-Attempts', String(streamResult.attempts.length + 1));

  // Now open SSE stream to client
  return streamSSE(c, async (stream) => {
    // Wire cleanup: when client disconnects, abort the upstream fetch
    stream.onAbort(() => {
      logger.debug(
        { provider: streamResult.providerId, model: streamResult.model },
        'Client disconnected, aborting upstream stream',
      );
      abortController.abort();
    });

    try {
      const reader = streamResult.response.body!.getReader();
      const decoder = new TextDecoder();
      const parser = createSSEParser();

      while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        const chunk = decoder.decode(value, { stream: true });
        const result = parser.parse(chunk);

        for (const data of result.events) {
          await stream.writeSSE({ data });
        }

        if (result.done) {
          // Write the final [DONE] marker to the client
          await stream.writeSSE({ data: '[DONE]' });
          break;
        }
      }
    } catch (error: unknown) {
      if (error instanceof Error && error.name === 'AbortError') {
        // Client disconnected -- clean exit, no error logging
        logger.debug(
          { provider: streamResult.providerId, model: streamResult.model },
          'Upstream stream aborted (client disconnect)',
        );
        return;
      }

      // Real error during streaming -- send error event to client
      const errorMessage = error instanceof Error ? error.message : String(error);
      logger.error(
        { provider: streamResult.providerId, model: streamResult.model, error: errorMessage },
        'Mid-stream error',
      );

      try {
        await stream.writeSSE({
          event: 'error',
          data: JSON.stringify({
            error: {
              message: `Stream error from provider: ${errorMessage}`,
              type: 'server_error',
              code: 'stream_error',
            },
          }),
        });
      } catch {
        // If we can't even write the error event, client is gone -- nothing to do
      }
    }
  });
}
```

4. For the non-streaming branch, clean up the existing code. Remove the `stream: _stream` destructuring warning and the `if (body.stream)` logger.warn block. The non-streaming path should now only execute when `stream` is falsy. Keep the existing non-streaming logic intact but update the destructuring:

```typescript
// Non-streaming path (existing logic)
const { model: _model, stream: _stream, ...cleanBody } = body;
const result = await executeChain(
  chain,
  cleanBody as ChatCompletionRequest,
  tracker,
  registry,
);
// ... rest unchanged
```

Important design decisions:
- The `executeStreamChain` call happens OUTSIDE `streamSSE()`. This means if all providers are exhausted, we return a JSON 503 error, NOT an empty SSE stream. This directly satisfies success criteria #2.
- The AbortController is created BEFORE `executeStreamChain` so that even if the client disconnects during the pre-stream waterfall phase, the upstream fetch is aborted.
- `stream.onAbort()` is registered FIRST inside the streamSSE callback, before any reads, to ensure cleanup is always registered.
- The `[DONE]` marker is forwarded to the client to match OpenAI SSE format.
- Mid-stream errors write an error event before the stream closes, so clients get a meaningful error rather than an abrupt disconnect.
  </action>
  <verify>
1. `npx tsc --noEmit` compiles cleanly
2. Manual verification: start the server (`npm run dev` or `npx tsx src/index.ts`) with a valid config
3. Non-streaming request still works: `curl -X POST http://localhost:3000/v1/chat/completions -H "Authorization: Bearer YOUR_KEY" -H "Content-Type: application/json" -d '{"model":"default","messages":[{"role":"user","content":"hi"}]}'` returns JSON response
4. Streaming request returns SSE: same curl but with `"stream": true` in body and add `--no-buffer` flag -- should see `data: {...}` lines followed by `data: [DONE]`
  </verify>
  <done>
- `stream: true` requests return text/event-stream SSE response with real-time chunks
- Pre-stream waterfall validates provider availability before opening SSE stream
- All-exhausted returns 503 JSON (not empty stream)
- Client disconnect triggers AbortController.abort() cleaning up upstream connection
- Mid-stream errors produce error SSE event and graceful close
- Non-streaming requests work exactly as before
- AbortError is handled silently (debug log only, no error spam)
  </done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` -- zero compilation errors
2. Non-streaming path unchanged: `stream: false` or omitted requests return JSON as before
3. Streaming path works: `stream: true` requests return `Content-Type: text/event-stream` with SSE chunks
4. Pre-stream waterfall: exhausted providers are skipped before stream opens
5. All-exhausted: returns 503 JSON error, NOT an empty SSE stream
6. Client disconnect: upstream fetch is aborted, no leaked connections
7. Mid-stream error: error event written to stream, server does not crash
8. AbortError: logged at debug level, not error level
</verification>

<success_criteria>
Phase 2 success criteria (all three):
1. A developer can send `stream: true` requests and receive real-time SSE chunks with no perceptible buffering delay
2. Waterfall routing works before streaming begins -- if the first provider is exhausted, the proxy skips to an available provider before starting the stream
3. When a client disconnects mid-stream, the proxy cleans up the upstream provider connection (no leaked connections or memory)

Additional criteria:
- Non-streaming requests are completely unaffected
- No new npm dependencies installed
- TypeScript compiles cleanly
</success_criteria>

<output>
After completion, create `.planning/phases/02-sse-streaming/02-02-SUMMARY.md`
</output>
