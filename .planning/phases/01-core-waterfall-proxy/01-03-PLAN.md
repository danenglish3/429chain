---
phase: 01-core-waterfall-proxy
plan: 03
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - src/ratelimit/types.ts
  - src/ratelimit/tracker.ts
  - src/ratelimit/cooldown.ts
  - src/chain/types.ts
  - src/chain/router.ts
autonomous: true

must_haves:
  truths:
    - "A provider+model marked exhausted is skipped without making a request"
    - "After cooldown expires, the provider+model is automatically re-enabled"
    - "The chain router iterates through entries in order, skipping exhausted ones, and returns the first successful response"
    - "When all providers fail, the router throws AllProvidersExhaustedError with a detailed attempt record for each entry"
    - "Non-429 provider errors (5xx, timeout, connection refused) also trigger waterfall to next entry"
  artifacts:
    - path: "src/ratelimit/types.ts"
      provides: "RateLimitState enum and CooldownEntry type"
      exports: ["RateLimitState", "CooldownEntry"]
    - path: "src/ratelimit/tracker.ts"
      provides: "In-memory rate limit tracker with exhausted/available state per provider+model"
      exports: ["RateLimitTracker"]
    - path: "src/ratelimit/cooldown.ts"
      provides: "Cooldown timer management with auto-recovery"
      exports: ["CooldownManager"]
    - path: "src/chain/types.ts"
      provides: "Chain and ChainEntry runtime types"
      exports: ["Chain", "ChainEntry"]
    - path: "src/chain/router.ts"
      provides: "Waterfall chain execution function"
      exports: ["executeChain"]
  key_links:
    - from: "src/chain/router.ts"
      to: "src/ratelimit/tracker.ts"
      via: "tracker.isExhausted() check before each attempt"
      pattern: "tracker\\.isExhausted"
    - from: "src/chain/router.ts"
      to: "src/providers/types.ts"
      via: "calls adapter.chatCompletion() through ProviderAdapter interface"
      pattern: "adapter\\.chatCompletion"
    - from: "src/ratelimit/tracker.ts"
      to: "src/ratelimit/cooldown.ts"
      via: "delegates timer management to CooldownManager"
      pattern: "cooldownManager"
    - from: "src/chain/router.ts"
      to: "src/shared/errors.ts"
      via: "throws AllProvidersExhaustedError when chain exhausted"
      pattern: "AllProvidersExhaustedError"
---

<objective>
Build the rate limit tracker (reactive 429 detection with cooldown timers) and the waterfall chain router that iterates through provider entries, skipping exhausted ones, and returns the first successful response.

Purpose: This is the core value proposition of 429chain -- when a provider fails, automatically try the next one. The rate limit tracker prevents wasting requests on providers known to be exhausted. Together, these components deliver the "requests never fail when free tokens exist somewhere" guarantee.
Output: A working chain router that waterfalls through providers and a rate limit tracker that manages cooldown state.
</objective>

<execution_context>
@C:\Users\danen\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\danen\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-core-waterfall-proxy/01-RESEARCH.md
@.planning/phases/01-core-waterfall-proxy/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Rate limit tracker and cooldown manager</name>
  <files>
    src/ratelimit/types.ts
    src/ratelimit/tracker.ts
    src/ratelimit/cooldown.ts
  </files>
  <action>
Create the reactive rate limit tracking system.

**src/ratelimit/types.ts**:
- `RateLimitState` enum or union type: `'available' | 'exhausted'`
- `CooldownEntry` interface: `{ providerId: string; model: string; status: RateLimitState; cooldownUntil: number | null; reason: string }`
- `TrackerEntry` interface (internal): `{ status: RateLimitState; cooldownUntil: number | null; reason: string }`

**src/ratelimit/cooldown.ts** -- CooldownManager:
- Manages setTimeout timers for auto-recovery
- `schedule(key: string, durationMs: number, onExpire: () => void): void` -- schedules a timer. If a timer already exists for this key, clears it first, then sets new one.
- `cancel(key: string): void` -- cancels and removes a timer
- `cancelAll(): void` -- cancels all timers (for graceful shutdown)
- `activeCount: number` getter -- number of active timers
- Uses a `Map<string, NodeJS.Timeout>` internally

**src/ratelimit/tracker.ts** -- RateLimitTracker:
- Private `state: Map<string, TrackerEntry>` -- keyed by `${providerId}:${model}`
- Private `cooldownManager: CooldownManager` instance
- Constructor takes `defaultCooldownMs: number` (from config.settings.cooldownDefaultMs)

- `isExhausted(providerId: string, model: string): boolean`:
  1. Get entry from state map
  2. If not found or status === 'available', return false
  3. If status === 'exhausted' AND cooldownUntil is set AND Date.now() >= cooldownUntil:
     - Call markAvailable (cooldown expired but timer didn't fire yet -- race condition safety)
     - Return false
  4. Return true

- `markExhausted(providerId: string, model: string, retryAfterMs?: number, reason?: string): void`:
  1. Compute key
  2. Compute cooldownMs = retryAfterMs || this.defaultCooldownMs
  3. Compute cooldownUntil = Date.now() + cooldownMs
  4. Set state entry: { status: 'exhausted', cooldownUntil, reason: reason || '429 rate limited' }
  5. Schedule cooldown timer via cooldownManager: when it fires, call markAvailable
  6. Log info: "Provider {providerId}/{model} exhausted, cooldown {cooldownMs}ms"

- `markAvailable(providerId: string, model: string): void`:
  1. Compute key
  2. Set state entry: { status: 'available', cooldownUntil: null, reason: '' }
  3. Cancel any pending timer via cooldownManager
  4. Log debug: "Provider {providerId}/{model} available again"

- `getStatus(providerId: string, model: string): CooldownEntry`:
  Returns the current state for monitoring/debugging.

- `getAllStatuses(): CooldownEntry[]`:
  Returns all tracked entries (for observability endpoints later).

- `shutdown(): void`:
  Calls cooldownManager.cancelAll() -- for graceful process shutdown.

IMPORTANT: The tracker uses a composite key of `${providerId}:${model}` because rate limits are per provider+model, not just per provider. A provider may have different rate limits for different models.
  </action>
  <verify>
1. `npx tsc --noEmit` passes.
2. Create a unit test: mark a provider exhausted with 100ms cooldown, verify isExhausted returns true, wait 150ms, verify isExhausted returns false.
3. Verify markExhausted replaces an existing timer (not accumulates).
  </verify>
  <done>
Rate limit tracker manages per-provider+model exhausted/available state. Cooldown manager handles setTimeout timers with auto-recovery. Double-check on expired cooldown prevents race conditions.
  </done>
</task>

<task type="auto">
  <name>Task 2: Chain types and waterfall router</name>
  <files>
    src/chain/types.ts
    src/chain/router.ts
  </files>
  <action>
Create the chain runtime types and the waterfall execution function.

**src/chain/types.ts**:
- `ChainEntry` interface: `{ providerId: string; model: string }` -- runtime representation of a chain entry (resolved from config)
- `Chain` interface: `{ name: string; entries: ChainEntry[] }` -- a named chain with ordered entries
- `ChainResult` interface: `{ response: ChatCompletionResponse; providerId: string; model: string; latencyMs: number; attempts: AttemptRecord[] }` -- successful result including which provider served it and what was tried
- `buildChains(config: Config, registry: ProviderRegistry): Map<string, Chain>` function:
  - Iterates config.chains, creates Chain objects
  - Validates each entry's provider exists in registry (should already be validated by Zod refine, but defensive check)
  - Returns a Map keyed by chain name

**src/chain/router.ts** -- Waterfall execution:
- `executeChain(chain: Chain, request: ChatCompletionRequest, tracker: RateLimitTracker, registry: ProviderRegistry): Promise<ChainResult>`:
  1. Initialize `attempts: AttemptRecord[]` array
  2. For each entry in chain.entries:
     a. Check `tracker.isExhausted(entry.providerId, entry.model)`:
        - If true: push `{ provider: entry.providerId, model: entry.model, error: 'on_cooldown', skipped: true }` to attempts, continue
     b. Get adapter via `registry.get(entry.providerId)`
     c. Try `adapter.chatCompletion(entry.model, request)`:
        - On success:
          - Parse rate limit headers: `adapter.parseRateLimitHeaders(result.headers)`
          - If rateLimitInfo exists and rateLimitInfo.remainingRequests is 0, proactively mark exhausted (edge case: last request before 429)
          - Return `{ response: result.body, providerId: entry.providerId, model: entry.model, latencyMs: result.latencyMs, attempts }`
        - On ProviderRateLimitError (429):
          - Extract retryAfterMs from error headers (check `retry-after` header, convert seconds to ms)
          - Call `tracker.markExhausted(entry.providerId, entry.model, retryAfterMs)`
          - Push attempt record with error: '429_rate_limited'
          - Continue to next entry
        - On ProviderError (non-429, e.g., 500, 502, 503):
          - Push attempt record with error: `${error.statusCode}: ${error.message}`
          - Continue to next entry (waterfall on ALL provider failures, not just 429)
        - On any other error (network timeout, DNS failure, connection refused):
          - Push attempt record with error: error.message
          - Continue to next entry
  3. After loop: throw `new AllProvidersExhaustedError(chain.name, attempts)`

- `resolveChain(chainName: string | undefined, chains: Map<string, Chain>, defaultChainName: string): Chain`:
  - If chainName provided, look it up in chains map. Throw if not found.
  - If not provided, use defaultChainName.
  - Returns the Chain object.

IMPORTANT implementation notes:
- The waterfall should continue on ANY provider error, not just 429. A provider returning 500 or timing out should not block the whole chain.
- The only errors that should NOT trigger waterfall are request-level errors (e.g., the incoming request is malformed). These are caught before the chain executes.
- Log each attempt (provider, model, success/failure, latency) at info level for observability.
- Log the overall chain result (which provider served, total attempts) at info level.
- The `request` passed to `chatCompletion` should NOT include the `model` field from the original request (the chain entry determines the model). Strip `model` from the incoming request and let the adapter set it. Actually -- the adapter's `chatCompletion` takes model as a separate param, so this is handled correctly.
  </action>
  <verify>
1. `npx tsc --noEmit` passes.
2. Write a unit test that mocks ProviderAdapter (using vitest mocking or a simple class implementing the interface):
   - First adapter throws ProviderRateLimitError, second adapter returns success -> verify ChainResult.providerId is the second adapter.
   - All adapters fail -> verify AllProvidersExhaustedError is thrown with correct attempt records.
   - First adapter is already exhausted in tracker -> verify it's skipped without calling chatCompletion.
  </verify>
  <done>
Chain router waterfalls through provider entries in order. Exhausted providers are skipped. On 429, provider is put on cooldown and next entry is tried. On any provider failure, next entry is tried. When all fail, AllProvidersExhaustedError is thrown with complete attempt history.
  </done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` passes with all ratelimit and chain files
2. Rate limit tracker correctly transitions between available and exhausted states
3. Cooldown auto-recovery fires after the configured duration
4. Chain router skips exhausted providers without making requests
5. Chain router waterfalls on 429, 5xx, timeout, and connection errors
6. AllProvidersExhaustedError contains accurate attempt records for all entries
7. Successful chain execution returns the response plus metadata about which provider served it
</verification>

<success_criteria>
- Rate limit tracker manages per-provider+model state with cooldown timers
- Cooldown expiry automatically re-enables providers
- Chain router iterates entries in order, skipping exhausted, waterfalling on any failure
- AllProvidersExhaustedError gives detailed info about every attempt
- No provider-specific logic in the chain router (all provider differences handled by adapters)
</success_criteria>

<output>
After completion, create `.planning/phases/01-core-waterfall-proxy/01-03-SUMMARY.md`
</output>
