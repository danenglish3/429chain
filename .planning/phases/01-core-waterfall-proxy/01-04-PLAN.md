---
phase: 01-core-waterfall-proxy
plan: 04
type: execute
wave: 3
depends_on: ["01-02", "01-03"]
files_modified:
  - src/api/middleware/auth.ts
  - src/api/middleware/error-handler.ts
  - src/api/routes/chat.ts
  - src/api/routes/models.ts
  - src/api/routes/health.ts
  - src/index.ts
autonomous: true

must_haves:
  truths:
    - "A developer can point an OpenAI SDK at http://localhost:3429 and get a non-streaming chat completion response"
    - "Requests without a valid API key in the Authorization header receive a 401 with OpenAI-format error"
    - "POST /v1/chat/completions routes through the waterfall chain and returns a standard ChatCompletionResponse"
    - "GET /v1/models returns models from all configured providers in OpenAI list format"
    - "GET /health returns 200 with proxy status"
    - "When all providers fail, the response is a detailed OpenAI-format error listing each provider and failure reason"
  artifacts:
    - path: "src/api/middleware/auth.ts"
      provides: "API key validation middleware for Hono"
      exports: ["authMiddleware"]
    - path: "src/api/middleware/error-handler.ts"
      provides: "Global error handler returning OpenAI-format errors"
      exports: ["errorHandler"]
    - path: "src/api/routes/chat.ts"
      provides: "POST /v1/chat/completions handler"
      exports: ["chatRoutes"]
    - path: "src/api/routes/models.ts"
      provides: "GET /v1/models handler"
      exports: ["modelsRoutes"]
    - path: "src/api/routes/health.ts"
      provides: "GET /health handler"
      exports: ["healthRoutes"]
    - path: "src/index.ts"
      provides: "Application entry point -- bootstraps Hono, loads config, starts server"
      exports: []
  key_links:
    - from: "src/api/routes/chat.ts"
      to: "src/chain/router.ts"
      via: "calls executeChain() with resolved chain"
      pattern: "executeChain"
    - from: "src/api/routes/chat.ts"
      to: "src/chain/router.ts"
      via: "calls resolveChain() to pick the right chain"
      pattern: "resolveChain"
    - from: "src/api/middleware/error-handler.ts"
      to: "src/shared/errors.ts"
      via: "catches AllProvidersExhaustedError and converts to OpenAI error format"
      pattern: "AllProvidersExhaustedError"
    - from: "src/index.ts"
      to: "src/config/loader.ts"
      via: "loadConfig() at startup"
      pattern: "loadConfig"
    - from: "src/index.ts"
      to: "src/providers/registry.ts"
      via: "buildRegistry() at startup"
      pattern: "buildRegistry"
    - from: "src/index.ts"
      to: "@hono/node-server"
      via: "serve() to start HTTP server"
      pattern: "serve"
---

<objective>
Build the HTTP layer: Hono application with API key auth middleware, OpenAI-compatible chat completions endpoint, models endpoint, health endpoint, error handler, and the entry point that wires everything together and starts the server.

Purpose: This is the user-facing surface of 429chain. After this plan, a developer can point an OpenAI SDK at the proxy and make requests. This completes Phase 1.
Output: A running HTTP proxy server with all Phase 1 endpoints and auth.
</objective>

<execution_context>
@C:\Users\danen\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\danen\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-core-waterfall-proxy/01-RESEARCH.md
@.planning/phases/01-core-waterfall-proxy/01-01-SUMMARY.md
@.planning/phases/01-core-waterfall-proxy/01-02-SUMMARY.md
@.planning/phases/01-core-waterfall-proxy/01-03-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Auth middleware, error handler, and route handlers</name>
  <files>
    src/api/middleware/auth.ts
    src/api/middleware/error-handler.ts
    src/api/routes/chat.ts
    src/api/routes/models.ts
    src/api/routes/health.ts
  </files>
  <action>
Create all middleware and route handlers for the Hono application.

**src/api/middleware/auth.ts** -- API key validation:
- Export `createAuthMiddleware(apiKeys: string[])` factory function that returns a Hono middleware.
- Uses `createMiddleware` from `hono/factory`.
- Logic:
  1. Extract `Authorization` header from request
  2. If missing or doesn't start with `Bearer `: return 401 JSON `{ error: { message: 'Missing or invalid API key. Provide a valid key in the Authorization header as Bearer <key>.', type: 'invalid_request_error', param: null, code: 'invalid_api_key' } }`
  3. Extract key (slice after 'Bearer ')
  4. If key not in apiKeys array: return 401 JSON `{ error: { message: 'Invalid API key provided.', type: 'invalid_request_error', param: null, code: 'invalid_api_key' } }`
  5. Call `await next()`
- NOTE: The /health endpoint should NOT require auth. The middleware is applied selectively to /v1/* routes only.

**src/api/middleware/error-handler.ts** -- Global error handler:
- Export `errorHandler` as a Hono error handler (using `app.onError` pattern).
- Logic:
  1. If error is `AllProvidersExhaustedError`: return 502 JSON with the error's `toOpenAIError()` output. Use 502 (Bad Gateway) because all upstream providers failed.
  2. If error is `ProviderRateLimitError`: return 429 JSON with OpenAI error format (this shouldn't normally happen since the chain router catches these, but defensive).
  3. If error is `ConfigError`: return 500 JSON `{ error: { message: 'Internal configuration error', type: 'server_error', param: null, code: 'config_error' } }`. Do NOT expose config details to client.
  4. For any other error: return 500 JSON `{ error: { message: 'Internal server error', type: 'server_error', param: null, code: null } }`. Log the full error at error level (but don't return it to client).
- Always log the error at appropriate level (warn for 4xx, error for 5xx).

**src/api/routes/chat.ts** -- POST /v1/chat/completions:
- Export a Hono route group (`new Hono()`) with POST handler at `/chat/completions`.
- The handler receives dependencies via Hono's context variables (set by the entry point).
- Handler logic:
  1. Parse request body as JSON
  2. Extract the `model` field from the request -- this is used as the chain name hint. If the request includes a `model` field that matches a chain name, use that chain. Otherwise, use the default chain. (This allows OpenAI SDK users to set `model: "fast"` to select the "fast" chain.) Log which chain is being used.
  3. Strip the `model` field from the request body before passing to the chain (the chain entry determines the actual model).
  4. Also strip `stream` field (force non-streaming in Phase 1). If the request had `stream: true`, log a warning: "Streaming not yet supported, falling back to non-streaming".
  5. Call `resolveChain(chainName, chains, defaultChainName)` to get the Chain.
  6. Call `executeChain(chain, request, tracker, registry)`.
  7. Return the `ChainResult.response` as JSON with status 200.
  8. Set response header `X-429chain-Provider` to `${result.providerId}/${result.model}` (so the caller can see which provider served the request).
  9. Set response header `X-429chain-Attempts` to the number of attempts (including skipped).
- Error handling: Let errors propagate to the error handler middleware. The chain router throws AllProvidersExhaustedError when all fail, which the error handler catches.

**src/api/routes/models.ts** -- GET /v1/models:
- Export a Hono route group with GET handler at `/models`.
- Handler logic:
  1. Get all chains from context
  2. Collect unique provider+model pairs from all chain entries
  3. Build response: `{ object: 'list', data: [...] }` where each entry is `{ id: model, object: 'model', created: Math.floor(Date.now() / 1000), owned_by: providerId }`
  4. Deduplicate by model ID (same model from different chains should appear once)
  5. Return as JSON with status 200

**src/api/routes/health.ts** -- GET /health:
- Export a Hono route group with GET handler at `/`.
- Returns `{ status: 'ok', version: '0.1.0', uptime: process.uptime(), providers: registry.size, chains: chains.size }` with status 200.
- No auth required.

IMPORTANT implementation notes:
- Use Hono's `c.set()` and `c.get()` or a shared context pattern to pass dependencies (config, registry, chains, tracker) to route handlers. One clean approach: create a custom Hono app type with typed variables, or use a factory pattern where route creators receive dependencies as closures.
- The recommended approach: create route factory functions that take dependencies and return Hono instances. For example: `createChatRoutes(chains, tracker, registry, defaultChainName)` returns a configured Hono route group.
- All responses must have `Content-Type: application/json` (Hono's `c.json()` handles this).
  </action>
  <verify>
1. `npx tsc --noEmit` passes.
2. Each route handler can be tested in isolation with mocked dependencies.
  </verify>
  <done>
Auth middleware rejects missing/invalid API keys with OpenAI-format 401. Error handler converts all error types to OpenAI-format JSON. Chat route resolves chain from model field and executes waterfall. Models route returns unique models from chains. Health route returns status without auth.
  </done>
</task>

<task type="auto">
  <name>Task 2: Application entry point and server bootstrap</name>
  <files>src/index.ts</files>
  <action>
Create the entry point that wires everything together and starts the HTTP server.

**src/index.ts**:
1. Import all modules: loadConfig, resolveConfigPath, buildRegistry, buildChains, RateLimitTracker, logger, createAuthMiddleware, errorHandler, route factories, serve from @hono/node-server, Hono.

2. Bootstrap sequence (all synchronous except server start):
   a. Log startup banner: "429chain v0.1.0 starting..."
   b. Resolve config path: `const configPath = resolveConfigPath()`
   c. Load and validate config: `const config = loadConfig(configPath)`
   d. Update logger level from config: set logger level to config.settings.logLevel
   e. Build provider registry: `const registry = buildRegistry(config.providers)`
   f. Build chains map: `const chains = buildChains(config, registry)`
   g. Create rate limit tracker: `const tracker = new RateLimitTracker(config.settings.cooldownDefaultMs)`

3. Create Hono app:
   a. `const app = new Hono()`
   b. Mount health route at root: `app.route('/health', healthRoutes)` (no auth)
   c. Create auth middleware: `const auth = createAuthMiddleware(config.settings.apiKeys)`
   d. Create a v1 sub-app with auth: `const v1 = new Hono()`, `v1.use('*', auth)`
   e. Mount chat routes on v1: `v1.route('/v1', chatRoutes)`
   f. Mount models routes on v1: `v1.route('/v1', modelsRoutes)`
   g. Mount v1 on app: `app.route('/', v1)`
   h. Set error handler: `app.onError(errorHandler)`

   Note: The exact Hono routing structure may need adjustment. The key requirement is:
   - `/health` is accessible without auth
   - `/v1/chat/completions` requires auth
   - `/v1/models` requires auth
   - All errors produce OpenAI-format JSON

4. Start server:
   ```typescript
   const server = serve({
     fetch: app.fetch,
     port: config.settings.port,
   }, (info) => {
     logger.info({ port: info.port }, `429chain listening on port ${info.port}`);
     logger.info({ providers: registry.size, chains: chains.size, defaultChain: config.settings.defaultChain }, 'Ready');
   });
   ```

5. Graceful shutdown:
   ```typescript
   const shutdown = () => {
     logger.info('Shutting down...');
     tracker.shutdown();  // Cancel all cooldown timers
     server.close(() => {
       logger.info('Server closed');
       process.exit(0);
     });
   };
   process.on('SIGINT', shutdown);
   process.on('SIGTERM', shutdown);
   ```

6. Unhandled rejection handler:
   ```typescript
   process.on('unhandledRejection', (reason) => {
     logger.error({ reason }, 'Unhandled rejection');
   });
   ```

IMPORTANT: The route factory approach means the entry point creates route handlers with dependencies injected, then mounts them on the Hono app. This keeps the dependency graph clear and makes testing easier.

The route factories created in Task 1 should accept their dependencies (chains, tracker, registry, etc.) either as constructor params or as closures. The entry point calls these factories with the real dependencies built from config.
  </action>
  <verify>
1. `npx tsc --noEmit` passes.
2. Create a test config file (copy config.example.yaml, fill in dummy keys).
3. Run `npx tsx src/index.ts --config config/config.yaml` -- server should start and log "listening on port 3429".
4. `curl http://localhost:3429/health` -- should return 200 with status JSON.
5. `curl http://localhost:3429/v1/models` without auth -- should return 401.
6. `curl -H "Authorization: Bearer test-key" http://localhost:3429/v1/models` (where test-key is in config) -- should return 200 with models list.
7. `curl -X POST -H "Authorization: Bearer test-key" -H "Content-Type: application/json" -d '{"model":"default","messages":[{"role":"user","content":"hi"}]}' http://localhost:3429/v1/chat/completions` -- should attempt to call providers (will fail with auth errors against real APIs unless keys are valid, but the waterfall should execute and return AllProvidersExhausted error in OpenAI format).
  </verify>
  <done>
Server starts, loads config, builds registry and chains, mounts routes with auth. Health endpoint works without auth. V1 endpoints require valid API key. Chat completions route through waterfall chain. All errors return OpenAI-format JSON. Graceful shutdown cancels cooldown timers and closes server.
  </done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` passes with all source files
2. Server starts with `npx tsx src/index.ts`
3. GET /health returns 200 without auth
4. GET /v1/models returns 401 without auth, 200 with valid auth
5. POST /v1/chat/completions with auth routes through chain
6. Invalid API key returns 401 OpenAI-format error
7. All providers failing returns 502 with detailed attempt listing
8. Graceful shutdown (Ctrl+C) logs "Shutting down" and exits cleanly
</verification>

<success_criteria>
- A developer can point an OpenAI SDK at the proxy and get a response (or a meaningful error) without changing client code
- Auth middleware protects /v1/* endpoints but not /health
- Error responses always use OpenAI error format ({ error: { message, type, param, code } })
- Server logs structured JSON with no API key leakage
- Graceful shutdown cleans up timers and connections
- Phase 1 is functionally complete: non-streaming waterfall proxy with auth, health, models, and detailed error reporting
</success_criteria>

<output>
After completion, create `.planning/phases/01-core-waterfall-proxy/01-04-SUMMARY.md`
</output>
